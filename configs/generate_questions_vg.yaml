image_folder: /net/acadia10a/data/zkhan/visual-genome-sandbox/image/
output_folder: /net/acadia10a/data/zkhan/vqav2_annotations/
annotations: null
pretrained: /net/acadia10a/data/zkhan/mithril/blip_vqg_2/checkpoint_04.pth
output_annotations_name: vg_generated_qa.json
image_size: 384
max_length: 30
min_length: 5
num_beams: 3
prompt: 'Question:'
torch_home: /net/acadia10a/data/zkhan/torch_home
vit: base
questions_per_image: 2
multimodal_encoder_decoder_config: /home/mai/zkhan/BLIP/configs/med_config.json
batch_size: 64
truncate_to: null
num_workers: 4
top_p: 0.9
vqa_dataset_origin: 'vg'
dry_run: false
shuffle: false

parse_rationale: false
